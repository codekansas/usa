{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7d398fd",
   "metadata": {},
   "source": [
    "# PyBullet demo\n",
    "\n",
    "This notebook demonstrates how to train a USA model on a PyBullet environment. The environment is taken from [here](https://github.com/adubredu/pybullet_kitchen).\n",
    "\n",
    "To get started, first make sure to install some dependencies:\n",
    "\n",
    "```bash\n",
    "pip install pybullet scikit-learn scipy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1307dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# These environment variables control where training and eval logs are written.\n",
    "# You can set these in your shell profile as well.\n",
    "os.environ[\"RUN_DIR\"] = \"runs\"\n",
    "os.environ[\"EVAL_RUN_DIR\"] = \"eval_runs\"\n",
    "os.environ[\"MODEL_DIR\"] = \"models\"\n",
    "os.environ[\"DATA_DIR\"] = \"data\"\n",
    "\n",
    "# This is used to set a constant Tensorboard port.\n",
    "os.environ[\"TENSORBOARD_PORT\"] = str(8989)\n",
    "\n",
    "import ml.api as ml  # Source: https://github.com/codekansas/ml-starter\n",
    "\n",
    "# Enables logging to `stdout`.\n",
    "ml.configure_logging(use_tqdm=True)\n",
    "\n",
    "# Imports these files to add them to the model and task registry.\n",
    "from usa.models.point2emb import Point2EmbModel\n",
    "from usa.tasks.clip_sdf import ClipSdfTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71396432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import ml.api as ml\n",
    "import numpy as np\n",
    "import pybullet as pb\n",
    "import requests\n",
    "from IPython.display import Image\n",
    "from omegaconf import OmegaConf\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d413870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pb.connect(pb.DIRECT)\n",
    "pb.resetSimulation()\n",
    "\n",
    "pb.setGravity(0, 0, -9.81)\n",
    "pb.setPhysicsEngineParameter(enableConeFriction=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ba4043",
   "metadata": {},
   "source": [
    "The code below downloads the environment data and adds it to PyBullet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b13bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path(\"data\")\n",
    "data_root.mkdir(exist_ok=True)\n",
    "\n",
    "# Downloads the dataset, if it is not already downloaded.\n",
    "if not (data_root / \"04_pybullet_data\").exists():\n",
    "    r = requests.get(\"https://github.com/codekansas/usa/releases/download/v0.0.2/04_pybullet_data.zip\", allow_redirects=True)\n",
    "    with open(data_root / \"04_pybullet_data.zip\", \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    with zipfile.ZipFile(data_root / \"04_pybullet_data.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_root)\n",
    "\n",
    "# Loads the URDFs into PyBullet.\n",
    "pb.setAdditionalSearchPath(str(data_root / \"04_pybullet_data\"))\n",
    "kitchen_path = \"kitchen_part_right_gen_convex.urdf\"\n",
    "use_fixed_base = True\n",
    "pb.setGravity(0, 0, -9.81)\n",
    "\n",
    "floor = pb.loadURDF(\n",
    "    \"floor.urdf\",\n",
    "    useFixedBase=use_fixed_base,\n",
    ")\n",
    "\n",
    "kitchen = pb.loadURDF(\n",
    "    \"kitchen_part_right_gen_convex.urdf\",\n",
    "    (-5, 0, 1.477),\n",
    "    useFixedBase=use_fixed_base,\n",
    ")\n",
    "\n",
    "table = pb.loadURDF(\n",
    "    \"table.urdf\",\n",
    "    (1.0, 0, 0),\n",
    "    pb.getQuaternionFromEuler((0, 0, 1.57)),\n",
    "    useFixedBase=use_fixed_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce77bdd",
   "metadata": {},
   "source": [
    "Next, we create a new Kitchen environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054efc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kitchen:\n",
    "    def __init__(self):\n",
    "        kitchen_path = \"kitchen_part_right_gen_convex.urdf\"\n",
    "        useFixedBase = True\n",
    "        pb.setGravity(0, 0, -9.81)\n",
    "        self.floor = pb.loadURDF(\"floor.urdf\", useFixedBase=useFixedBase)\n",
    "        self.kitchen = pb.loadURDF(kitchen_path, [-5, 0, 1.477], useFixedBase=useFixedBase)\n",
    "        self.table = pb.loadURDF(\n",
    "            \"table.urdf\",\n",
    "            [1.0, 0, 0],\n",
    "            pb.getQuaternionFromEuler([0, 0, 1.57]),\n",
    "            useFixedBase=useFixedBase,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b38a35a",
   "metadata": {},
   "source": [
    "Next, let's collect some samples from the environment. The `PosedRGBDItem` used for training the model has the following description:\n",
    "\n",
    "```python\n",
    "class PosedRGBDItem(NamedTuple):\n",
    "    image: Tensor       # RGB image, with shape (C, H, W)\n",
    "    depth: Tensor       # Depth image, with shape (1, H, W)\n",
    "    mask: Tensor        # Valid depth points, with shape (1, H, W), where True means valid\n",
    "    intrinsics: Tensor  # Camera intrinsics matrix, with shape (3, 3)\n",
    "    pose: Tensor        # Camera pose matrix, with shape (4, 4)\n",
    "\n",
    "    def check(self) -> None:\n",
    "        # Image should have shape (C, H, W)\n",
    "        assert self.image.dim() == 3\n",
    "        assert self.image.dtype == torch.float32\n",
    "        # Depth should have shape (1, H, W)\n",
    "        assert self.depth.dim() == 3\n",
    "        assert self.depth.shape[0] == 1\n",
    "        assert self.depth.dtype == torch.float32\n",
    "        # Depth shape should match image shape.\n",
    "        assert self.depth.shape[1:] == self.image.shape[1:]\n",
    "        assert self.mask.shape[1:] == self.image.shape[1:]\n",
    "        # Intrinsics should have shape (3, 3)\n",
    "        assert self.intrinsics.shape == (3, 3)\n",
    "        assert self.intrinsics.dtype == torch.float64\n",
    "        # Pose should have shape (4, 4)\n",
    "        assert self.pose.shape == (4, 4)\n",
    "        assert self.pose.dtype == torch.float64\n",
    "```\n",
    "\n",
    "The intrinsics matrix can be constructed as follows:\n",
    "\n",
    "```python\n",
    "def intrinsics_matrix(fx: float, fy: float, cx: float, cy: float) -> np.ndarray:\n",
    "    intr = np.eye(3, dtype=np.float64)\n",
    "    intr[0, 0] = fx\n",
    "    intr[1, 1] = fy\n",
    "    intr[0, 2] = cx\n",
    "    intr[1, 2] = cy\n",
    "    return intr\n",
    "```\n",
    "\n",
    "The pose matrix can be constructed from the rotation quaternion and translation vector using this snippet:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from quaternion import as_rotation_matrix, quaternion  # pip install numpy-quaternion\n",
    "\n",
    "\n",
    "def as_pose_matrix(pose: list[float]) -> np.ndarray:\n",
    "    \"\"\"Converts a list of pose parameters to a pose matrix.\n",
    "\n",
    "    Args:\n",
    "        pose: The list of pose parameters, (qx, qy, qz, qw, px, py, pz)\n",
    "\n",
    "    Returns:\n",
    "        A (4, 4) pose matrix\n",
    "    \"\"\"\n",
    "\n",
    "    mat = np.eye(4, dtype=np.float64)\n",
    "    qx, qy, qz, qw, px, py, pz = pose\n",
    "    mat[:3, :3] = as_rotation_matrix(quaternion(qw, qx, qy, qz))\n",
    "    mat[:3, 3] = [px, py, pz]\n",
    "    return mat\n",
    "```\n",
    "\n",
    "In the example below, we move the camera around the center point to collect a few frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24577dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_frame(\n",
    "    camera_xyz: tuple[float, float, float] = (-5.0, 0.0, 1.477),\n",
    "    camera_ypr: tuple[float, float, float] = (90.0, -10.0, 0.0),\n",
    "    camera_distance: float = 3.0,\n",
    "    camera_planes: tuple[float, float] = (0.01, 10.0),\n",
    "    pixel_dims: tuple[int, int] = (500, 300),\n",
    "    camera_fov: float = 60.0,\n",
    ") -> tuple[np.ndarray, ...]:\n",
    "    \"\"\"Captures a single frame, returning RGB and depth information.\n",
    "\n",
    "    Args:\n",
    "        camera_xyz: The XYZ coordinates of the camera\n",
    "        camera_ypr: The yaw, pitch and roll of the camera\n",
    "        cam_distance: Not sure\n",
    "        camera_planes: The minimum and maximum rendering distances\n",
    "        pixel_dims: The shape of the output image, as (W, H)\n",
    "        camera_fov: The camera field of view\n",
    "        \n",
    "    Returns:\n",
    "        The RGB image with shape (H, W, 3), the depth image with shape (H, W),\n",
    "        the intrinsics matrix with shape (3, 3), and the pose matrix with\n",
    "        shape (4, 4).\n",
    "    \"\"\"\n",
    "\n",
    "    yaw, pitch, roll = camera_ypr\n",
    "    near_plane, far_plane = camera_planes\n",
    "    pixel_width, pixel_height = pixel_dims\n",
    "\n",
    "    # Computes the view and projection matrices.\n",
    "    view_mat = pb.computeViewMatrixFromYawPitchRoll(camera_xyz, camera_distance, yaw, pitch, roll, 2)\n",
    "    aspect = pixel_width / pixel_height\n",
    "    proj_mat = pb.computeProjectionMatrixFOV(camera_fov, aspect, near_plane, far_plane)\n",
    "\n",
    "    # Captures the camera image.\n",
    "    img_arr = pb.getCameraImage(pixel_width, pixel_height, view_mat, proj_mat)\n",
    "    img_width, img_height, rgb, depth, *_ = img_arr\n",
    "\n",
    "    # Reshapes arrays to expected output shape.\n",
    "    rgb_arr = np.reshape(rgb, (img_height, img_width, 4))[..., :3]\n",
    "    depth_arr = np.reshape(depth, (img_height, img_width))\n",
    "    \n",
    "    # Converts depth to true depth.\n",
    "    depth_arr = far_plane * near_plane / (far_plane - (far_plane - near_plane) * depth_arr)\n",
    "    \n",
    "    # Gets camera intrinsics matrix.\n",
    "    fx = pixel_width / (2 * np.tan(np.deg2rad(camera_fov) / 2))\n",
    "    fy = pixel_height / (2 * np.tan(np.deg2rad(camera_fov) / 2))\n",
    "    cx = pixel_width / 2\n",
    "    cy = pixel_height / 2\n",
    "    intr = np.eye(3)\n",
    "    intr[0, 0] = fx\n",
    "    intr[1, 1] = fy\n",
    "    intr[0, 2] = cx\n",
    "    intr[1, 2] = cy\n",
    "    \n",
    "    # Gets the pose matrix.\n",
    "    pose = np.linalg.inv(np.array(view_mat).reshape(4, 4))\n",
    "    \n",
    "    return rgb_arr, depth_arr, intr, pose\n",
    "\n",
    "\n",
    "def move_camera(\n",
    "    camera_xyz: tuple[float, float, float],\n",
    "    camera_ypr: tuple[float, float, float],\n",
    ") -> tuple[tuple[float, float, float], tuple[float, float, float]]:\n",
    "    x, y, z = camera_xyz\n",
    "    yaw, pitch, roll = camera_ypr\n",
    "    return (\n",
    "        (x - 0.005, y + 0.01, z),\n",
    "        (yaw + 0.1, pitch, roll),\n",
    "    )\n",
    "\n",
    "\n",
    "def capture_sim(total_steps: int = 100, capture_every: int = 1) -> Iterator[tuple[np.ndarray, ...]]:\n",
    "    camera_xyz = (-5.0, 0.0, 1.477)\n",
    "    camera_ypr = (90.0, -10.0, 0.0)\n",
    "    for i in range(total_steps):\n",
    "        camera_xyz, camera_ypr = move_camera(camera_xyz, camera_ypr)\n",
    "        if i % capture_every == 0:\n",
    "            yield capture_frame(camera_xyz, camera_ypr)\n",
    "\n",
    "\n",
    "def write_gif(frames: Iterator[tuple[np.ndarray, ...]], out_file: str | Path, pkl_file: str | Path, *, fps: int = 30) -> None:\n",
    "    rgb, depth, mask, poses, intrinsics = [], [], [], [], []\n",
    "\n",
    "    writer = imageio.get_writer(str(out_file), mode=\"I\", fps=fps)\n",
    "    for rgb_frame, depth_frame, intr, pose in frames:\n",
    "        # Adds to the lists.\n",
    "        rgb.append(rgb_frame)\n",
    "        depth.append(depth_frame)\n",
    "        mask.append(depth_frame < 7.0)\n",
    "        poses.append(pose)\n",
    "        intrinsics.append(intr)\n",
    "\n",
    "        # Adds the image to the GIF.\n",
    "        depth_normalized = (depth_frame - np.min(depth_frame)) / (np.max(depth_frame) - np.min(depth_frame) + 1e-3)\n",
    "        depth_colorized = (plt.cm.jet(depth_normalized)[..., :3] * 255).astype(np.uint8)\n",
    "        frame = np.concatenate([rgb_frame, depth_colorized], axis=0)\n",
    "        writer.append_data(frame)\n",
    "\n",
    "    # Saves the pickle file.\n",
    "    data = {\n",
    "        \"rgb\": np.stack(rgb),\n",
    "        \"depth\": np.stack(depth),\n",
    "        \"mask\": np.stack(mask),\n",
    "        \"poses\": np.stack(poses),\n",
    "        \"intrinsics\": np.stack(intrinsics),\n",
    "    }\n",
    "    with open(pkl_file, \"wb\") as f:\n",
    "        pkl.dump(data, f)\n",
    "\n",
    "    # Closes the writer.\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def iter_frames() -> Iterator[np.ndarray]:\n",
    "    pb.resetSimulation()\n",
    "    kitchen = Kitchen()\n",
    "    yield from capture_sim()\n",
    "\n",
    "\n",
    "pkl_path = data_root / \"04_recorded_clip.pkl\"\n",
    "write_gif(iter_frames(), \"video.gif\", pkl_path)\n",
    "Image(\"video.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a0c0e",
   "metadata": {},
   "source": [
    "Next, we can train a model on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the default config, but overriding the dataset.\n",
    "config = OmegaConf.load(\"config.yaml\")\n",
    "config.task.dataset = \"pybullet\"\n",
    "config.task.dataset_path = str(pkl_path)\n",
    "\n",
    "# We still need to explicitly set these variables.\n",
    "config.trainer.exp_name = \"jupyter\"\n",
    "config.trainer.log_dir_name = \"test\"\n",
    "config.trainer.base_run_dir = \"runs\"\n",
    "config.trainer.run_id = 0\n",
    "\n",
    "# You can toggle this to test out by training a model for a shorter amount of time.\n",
    "# config.task.finished.max_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79033a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "objs = ml.instantiate_config(config)\n",
    "\n",
    "# Unpacking the different components.\n",
    "model = objs.model\n",
    "task = objs.task\n",
    "optimizer = objs.optimizer\n",
    "lr_scheduler = objs.lr_scheduler\n",
    "trainer = objs.trainer\n",
    "\n",
    "from tensorboard import notebook\n",
    "\n",
    "# Show Tensorboard inside the notebook.\n",
    "notebook.display(port=int(os.environ['TENSORBOARD_PORT']))\n",
    "\n",
    "# Runs the training loop.\n",
    "trainer.train(model, task, optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424abf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a model has already been trained, you can load trained model and task\n",
    "# using this line:\n",
    "# model, task = ml.load_model_and_task(\"runs/jupyter/run_0/config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a396be5d",
   "metadata": {},
   "source": [
    "After the model has been trained, we can load it and visualize some of the planned trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4eb541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from usa.planners.clip_sdf import GradientPlanner\n",
    "\n",
    "# Builds the planner from the model and task.\n",
    "planner = GradientPlanner(\n",
    "    dataset=task._dataset,\n",
    "    model=model,\n",
    "    task=task,\n",
    "    device=task._device,\n",
    "\n",
    "    # The learning rate for the optimizer for the waypoints\n",
    "    lr=1e-2,\n",
    "    # The weight for the total path distance loss term\n",
    "    dist_loss_weight=1.0,\n",
    "    # The weight for the inter-point distance loss term\n",
    "    spacing_loss_weight=1.0,\n",
    "    # The weight for the \"no-crashing-into-a-wall\" loss term\n",
    "    occ_loss_weight=25.0,\n",
    "    # The weight for the loss term of the final semantic location\n",
    "    sim_loss_weight=15.0,\n",
    "    # Maximum number of optimization steps\n",
    "    num_optimization_steps=1000,\n",
    "    # If points move less than this distance, stop optimizing\n",
    "    min_distance=1e-5,\n",
    "    # Where to store cache artifacts\n",
    "    # cache_dir=Path(\"cache\"),\n",
    "    cache_dir=None,\n",
    "    # Height of the floor\n",
    "    floor_height=0.1,\n",
    "    # Height of the ceiling\n",
    "    ceil_height=2.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd9d0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# waypoints = [(-5.0 + i * 0.1, 0.0) for i in range(20)]\n",
    "waypoints = planner.plan(start_xy=(-5.0, 0.0), end_xy=(-1.0, 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c260cdf5",
   "metadata": {},
   "source": [
    "Now that we've got our trajectory, we can run it in simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ecb81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_camera(\n",
    "    camera_xyz: tuple[float, float, float],\n",
    "    camera_ypr: tuple[float, float, float],\n",
    ") -> tuple[tuple[float, float, float], tuple[float, float, float]]:\n",
    "    x, y, z = camera_xyz\n",
    "    yaw, pitch, roll = camera_ypr\n",
    "    return (\n",
    "        (x - 0.005, y + 0.01, z),\n",
    "        (yaw + 0.1, pitch, roll),\n",
    "    )\n",
    "\n",
    "\n",
    "def capture_sim(total_steps: int = 100, capture_every: int = 1) -> Iterator[tuple[np.ndarray, ...]]:\n",
    "    camera_xyz = (-5.0, 0.0, 1.477)\n",
    "    camera_ypr = (90.0, -10.0, 0.0)\n",
    "    for i in range(total_steps):\n",
    "        camera_xyz, camera_ypr = move_camera(camera_xyz, camera_ypr)\n",
    "        if i % capture_every == 0:\n",
    "            yield capture_frame(camera_xyz, camera_ypr)\n",
    "\n",
    "\n",
    "def iter_frames_for_waypoints(waypoints: list[tuple[float, float]], *, camera_z: float = 1.477, camera_ypr: tuple[float, float, float] = (90.0, -10.0, 0.0)) -> Iterator[np.ndarray]:\n",
    "    pb.resetSimulation()\n",
    "    kitchen = Kitchen()\n",
    "    for x, y in waypoints:\n",
    "        yield capture_frame((x, y, camera_z), camera_ypr)\n",
    "\n",
    "\n",
    "pkl_path = data_root / \"04_recorded_clip_waypoints.pkl\"\n",
    "write_gif(iter_frames_for_waypoints(waypoints), \"waypoints_video.gif\", pkl_path)\n",
    "Image(\"waypoints_video.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
