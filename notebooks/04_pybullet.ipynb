{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7d398fd",
   "metadata": {},
   "source": [
    "# PyBullet demo\n",
    "\n",
    "This notebook demonstrates how to train a USA model on a PyBullet environment. The environment is taken from [here](https://github.com/adubredu/pybullet_kitchen).\n",
    "\n",
    "To get started, first make sure to install some dependencies:\n",
    "\n",
    "```bash\n",
    "pip install pybullet scikit-learn scipy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1307dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# These environment variables control where training and eval logs are written.\n",
    "# You can set these in your shell profile as well.\n",
    "os.environ[\"RUN_DIR\"] = \"runs\"\n",
    "os.environ[\"EVAL_RUN_DIR\"] = \"eval_runs\"\n",
    "os.environ[\"MODEL_DIR\"] = \"models\"\n",
    "os.environ[\"DATA_DIR\"] = \"data\"\n",
    "\n",
    "# This is used to set a constant Tensorboard port.\n",
    "os.environ[\"TENSORBOARD_PORT\"] = str(8989)\n",
    "\n",
    "# Useful for debugging.\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import ml.api as ml  # Source: https://github.com/codekansas/ml-starter\n",
    "\n",
    "ml.configure_logging()\n",
    "\n",
    "# Imports these files to add them to the model and task registry.\n",
    "from usa.models.point2emb import Point2EmbModel\n",
    "from usa.tasks.clip_sdf import ClipSdfTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71396432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle as pkl\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import ml.api as ml\n",
    "import numpy as np\n",
    "import pybullet as pb\n",
    "import requests\n",
    "import torch\n",
    "from IPython.display import Image\n",
    "from omegaconf import OmegaConf\n",
    "from pyquaternion import Quaternion\n",
    "from scipy.spatial.transform import Rotation\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d413870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pb.connect(pb.DIRECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ba4043",
   "metadata": {},
   "source": [
    "The code below downloads the environment data and adds it to PyBullet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b13bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path(\"data\")\n",
    "data_root.mkdir(exist_ok=True)\n",
    "\n",
    "# Downloads the dataset, if it is not already downloaded.\n",
    "if not (data_root / \"04_pybullet_data\").exists():\n",
    "    r = requests.get(\"https://github.com/codekansas/usa/releases/download/v0.0.2/04_pybullet_data.zip\", allow_redirects=True)\n",
    "    with open(data_root / \"04_pybullet_data.zip\", \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    with zipfile.ZipFile(data_root / \"04_pybullet_data.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_root)\n",
    "\n",
    "# Loads the URDFs into PyBullet.\n",
    "pb.setAdditionalSearchPath(str(data_root / \"04_pybullet_data\"))\n",
    "kitchen_path = \"kitchen_part_right_gen_convex.urdf\"\n",
    "use_fixed_base = True\n",
    "pb.setGravity(0, 0, -9.81)\n",
    "\n",
    "def reset_simulation() -> None:\n",
    "    pb.resetSimulation()\n",
    "    pb.setGravity(0, 0, -9.81)\n",
    "    pb.setPhysicsEngineParameter(enableConeFriction=0)\n",
    "    \n",
    "    floor = pb.loadURDF(\n",
    "        \"floor.urdf\",\n",
    "        useFixedBase=use_fixed_base,\n",
    "    )\n",
    "\n",
    "    kitchen = pb.loadURDF(\n",
    "        \"kitchen_part_right_gen_convex.urdf\",\n",
    "        (0.0, 0, 1.477),\n",
    "        useFixedBase=use_fixed_base,\n",
    "    )\n",
    "\n",
    "    table = pb.loadURDF(\n",
    "        \"table.urdf\",\n",
    "        (2.5, 0, 0),\n",
    "        pb.getQuaternionFromEuler((0, 0, 1.57)),\n",
    "        useFixedBase=use_fixed_base,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b38a35a",
   "metadata": {},
   "source": [
    "Next, let's collect some samples from the environment. The `PosedRGBDItem` used for training the model has the following description:\n",
    "\n",
    "```python\n",
    "class PosedRGBDItem(NamedTuple):\n",
    "    image: Tensor       # RGB image, with shape (C, H, W)\n",
    "    depth: Tensor       # Depth image, with shape (1, H, W)\n",
    "    mask: Tensor        # Valid depth points, with shape (1, H, W), where True means valid\n",
    "    intrinsics: Tensor  # Camera intrinsics matrix, with shape (3, 3)\n",
    "    pose: Tensor        # Camera pose matrix, with shape (4, 4)\n",
    "\n",
    "    def check(self) -> None:\n",
    "        # Image should have shape (C, H, W)\n",
    "        assert self.image.dim() == 3\n",
    "        assert self.image.dtype == torch.float32\n",
    "        # Depth should have shape (1, H, W)\n",
    "        assert self.depth.dim() == 3\n",
    "        assert self.depth.shape[0] == 1\n",
    "        assert self.depth.dtype == torch.float32\n",
    "        # Depth shape should match image shape.\n",
    "        assert self.depth.shape[1:] == self.image.shape[1:]\n",
    "        assert self.mask.shape[1:] == self.image.shape[1:]\n",
    "        # Intrinsics should have shape (3, 3)\n",
    "        assert self.intrinsics.shape == (3, 3)\n",
    "        assert self.intrinsics.dtype == torch.float64\n",
    "        # Pose should have shape (4, 4)\n",
    "        assert self.pose.shape == (4, 4)\n",
    "        assert self.pose.dtype == torch.float64\n",
    "```\n",
    "\n",
    "In the example below, we move the camera around the center point to collect a few frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24577dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_frame(\n",
    "    camera_xyz: tuple[float, float, float] = (-5.0, 0.0, 1.477),\n",
    "    camera_ypr: tuple[float, float, float] = (90.0, -10.0, 0.0),\n",
    "    camera_planes: tuple[float, float] = (0.01, 10.0),\n",
    "    pixel_dims: tuple[int, int] = (500, 300),\n",
    "    camera_fov: float = 80.0,\n",
    ") -> tuple[np.ndarray, ...]:\n",
    "    \"\"\"Captures a single frame, returning RGB and depth information.\n",
    "\n",
    "    Args:\n",
    "        camera_xyz: The XYZ coordinates of the camera\n",
    "        camera_ypr: The yaw, pitch and roll of the camera\n",
    "        camera_planes: The minimum and maximum rendering distances\n",
    "        pixel_dims: The shape of the output image, as (W, H)\n",
    "        camera_fov: The camera field of view\n",
    "        \n",
    "    Returns:\n",
    "        The RGB image with shape (H, W, 3), the depth image with shape (H, W),\n",
    "        the intrinsics matrix with shape (3, 3), and the pose matrix with\n",
    "        shape (4, 4).\n",
    "    \"\"\"\n",
    "    \n",
    "    x, y, z = camera_xyz\n",
    "    yaw, pitch, roll = camera_ypr\n",
    "    near_plane, far_plane = camera_planes\n",
    "    pixel_width, pixel_height = pixel_dims\n",
    "\n",
    "    # Computes the view and projection matrices.\n",
    "    view_mat = pb.computeViewMatrixFromYawPitchRoll(camera_xyz, near_plane, yaw, pitch, roll, 2)\n",
    "    aspect = pixel_width / pixel_height\n",
    "    proj_mat = pb.computeProjectionMatrixFOV(camera_fov, aspect, near_plane, far_plane)\n",
    "\n",
    "    # Captures the camera image.\n",
    "    img_arr = pb.getCameraImage(\n",
    "        width=pixel_width,\n",
    "        height=pixel_height,\n",
    "        viewMatrix=view_mat,\n",
    "        projectionMatrix=proj_mat,\n",
    "    )\n",
    "    img_width, img_height, rgb, depth, info = img_arr\n",
    "\n",
    "    # Reshapes arrays to expected output shape.\n",
    "    rgb_arr = np.reshape(rgb, (img_height, img_width, 4))[..., :3]\n",
    "    depth_arr = np.reshape(depth, (img_height, img_width))\n",
    "    \n",
    "    # Converts depth to true depth.\n",
    "    depth_arr = far_plane * near_plane / (far_plane - (far_plane - near_plane) * depth_arr)\n",
    "    \n",
    "    # Gets camera intrinsics matrix.\n",
    "    cx = pixel_width / 2\n",
    "    cy = pixel_height / 2\n",
    "    fov_rad = np.deg2rad(camera_fov)\n",
    "    fx = cx / np.tan(fov_rad / 2)\n",
    "    fy = cy / np.tan(fov_rad / 2)\n",
    "    \n",
    "    \"\"\"\n",
    "    proj_mat_np = np.array(proj_mat, dtype=np.float64).reshape(4, 4, order=\"F\")\n",
    "    fx = proj_mat_np[0, 0]\n",
    "    fy = proj_mat_np[1, 1]\n",
    "    cx = proj_mat_np[0, 2]\n",
    "    cy = proj_mat_np[1, 2]\n",
    "    \"\"\"\n",
    "    \n",
    "    intr = np.eye(3)\n",
    "    intr[0, 0] = fx\n",
    "    intr[1, 1] = fy\n",
    "    intr[0, 2] = cx\n",
    "    intr[1, 2] = cy\n",
    "    \n",
    "    # Gets poses from view matrix.\n",
    "    pose = np.linalg.inv(np.array(view_mat, dtype=np.float64).reshape(4, 4, order=\"F\"))\n",
    "    affine_mat = np.array(\n",
    "        [\n",
    "            [1, 0, 0, 0],\n",
    "            [0, -1, 0, 0],\n",
    "            [0, 0, -1, 0],\n",
    "            [0, 0, 0, 1],\n",
    "        ]\n",
    "    )\n",
    "    pose = pose @ affine_mat\n",
    "    \n",
    "    return rgb_arr, depth_arr, intr, pose\n",
    "\n",
    "\n",
    "def capture_sim(capture_every: int = 1) -> Iterator[tuple[np.ndarray, ...]]:\n",
    "    # xyz, ypr = (-5.0, 0.0, 1.477), (90.0, -10.0, 0.0)\n",
    "    for i in range(90):\n",
    "        degs = i * 4\n",
    "        rads = np.deg2rad(degs)\n",
    "        dist = 3.0\n",
    "        xyz = (dist * np.cos(rads), dist * np.sin(rads), 1.477)\n",
    "        # xyz = (0.0, 0.0, 1.477 + np.sin(rads) * 0.5)\n",
    "        # xyz = (0.0, 0.0, 1.477)\n",
    "        # ypr = (degs, -10.0 + np.sin(rads) * 10.0, 0.0)\n",
    "        ypr = (degs + 90.0, -10.0, 0.0)\n",
    "        if i % capture_every == 0:\n",
    "            yield capture_frame(xyz, ypr)\n",
    "\n",
    "\n",
    "def write_gif(frames: Iterator[tuple[np.ndarray, ...]], out_file: str | Path, pkl_file: str | Path, *, fps: int = 30) -> None:\n",
    "    rgb, depth, mask, poses, intrinsics = [], [], [], [], []\n",
    "\n",
    "    writer = imageio.get_writer(str(out_file), mode=\"I\", fps=fps)\n",
    "    for rgb_frame, depth_frame, intr, pose in frames:\n",
    "        # Adds to the lists.\n",
    "        rgb.append(rgb_frame)\n",
    "        depth.append(depth_frame)\n",
    "        mask.append(depth_frame > 7.0)\n",
    "        # mask.append(np.zeros_like(depth_frame, dtype=np.bool_))\n",
    "        poses.append(pose)\n",
    "        intrinsics.append(intr)\n",
    "\n",
    "        # Adds the image to the GIF.\n",
    "        depth_normalized = (depth_frame - np.min(depth_frame)) / (np.max(depth_frame) - np.min(depth_frame) + 1e-3)\n",
    "        depth_colorized = (plt.cm.jet(depth_normalized)[..., :3] * 255).astype(np.uint8)\n",
    "        frame = np.concatenate([rgb_frame, depth_colorized], axis=0)\n",
    "        writer.append_data(frame)\n",
    "\n",
    "    # Saves the pickle file.\n",
    "    data = {\n",
    "        \"rgb\": np.stack(rgb),\n",
    "        \"depth\": np.stack(depth),\n",
    "        \"mask\": np.stack(mask),\n",
    "        \"poses\": np.stack(poses),\n",
    "        \"intrinsics\": np.stack(intrinsics),\n",
    "    }\n",
    "    with open(pkl_file, \"wb\") as f:\n",
    "        pkl.dump(data, f)\n",
    "\n",
    "    # Closes the writer.\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def iter_frames() -> Iterator[np.ndarray]:\n",
    "    reset_simulation()\n",
    "    yield from capture_sim()\n",
    "\n",
    "\n",
    "pkl_path = data_root / \"04_recorded_clip.pkl\"\n",
    "write_gif(iter_frames(), \"video.gif\", pkl_path)\n",
    "\n",
    "Image(\"video.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b52503",
   "metadata": {},
   "source": [
    "We can visualize the point cloud for the dataset using the snippet below. Note that this requires `pythreejs`, which can be installed using:\n",
    "\n",
    "```bash\n",
    "pip install pythreejs\n",
    "```\n",
    "\n",
    "Also, if using a normal Jupyter notebook, you will need to enable the extension using the code below (see the project page [here](https://github.com/jupyter-widgets/pythreejs)):\n",
    "\n",
    "```bash\n",
    "jupyter nbextension list\n",
    "jupyter nbextension install --py --symlink --sys-prefix pythreejs\n",
    "jupyter nbextension enable --py --sys-prefix pythreejs\n",
    "jupyter nbextension list\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2351da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from usa.tasks.datasets.utils import visualize_posed_rgbd_dataset\n",
    "from usa.tasks.datasets.pybullet import PyBulletDataset\n",
    "\n",
    "# Creates a new dataset from the recorded pickle file.\n",
    "dataset = PyBulletDataset(path=pkl_path)\n",
    "\n",
    "# Creates a point cloud of the dataset.\n",
    "out_path = Path(\"out/point_cloud.ply\")\n",
    "visualize_posed_rgbd_dataset(\n",
    "    dataset,\n",
    "    make_video=False,\n",
    "    make_point_cloud=True,\n",
    "    max_point_cloud_samples=2,\n",
    "    point_cloud_sample_stride=5,\n",
    "    output_dir=out_path.parent,\n",
    ")\n",
    "\n",
    "if False:  # This is just for testing\n",
    "    # Plots the point cloud using PyVista.\n",
    "    import pyvista as pv\n",
    "    pv.set_jupyter_backend(\"pythreejs\")\n",
    "    cloud = pv.read(\"out/point_cloud.ply\")\n",
    "    cloud.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32d46b6",
   "metadata": {},
   "source": [
    "Next, we train a model on the recorded clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the default config, but overriding the dataset.\n",
    "config = OmegaConf.load(\"config.yaml\")\n",
    "config.task.dataset = \"pybullet\"\n",
    "config.task.dataset_path = str(pkl_path)\n",
    "\n",
    "# We still need to explicitly set these variables.\n",
    "config.trainer.exp_name = \"jupyter\"\n",
    "config.trainer.base_run_dir = \"runs\"\n",
    "config.trainer.run_id = 0\n",
    "\n",
    "# Only use stdout logger.\n",
    "config.logger = [{\"name\": \"stdout\"}]\n",
    "\n",
    "# You can change this number to change the number of training steps.\n",
    "config.task.finished.max_steps = 2500\n",
    "\n",
    "# Loads the config objects.\n",
    "objs = ml.instantiate_config(config)\n",
    "\n",
    "# Unpacking the different components.\n",
    "model = objs.model\n",
    "task = objs.task\n",
    "optimizer = objs.optimizer\n",
    "lr_scheduler = objs.lr_scheduler\n",
    "trainer = objs.trainer\n",
    "\n",
    "# Runs the training loop.\n",
    "trainer.train(model, task, optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424abf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a model has already been trained, you can load trained model and task\n",
    "# using this line:\n",
    "# model, task = ml.load_model_and_task(\"runs/jupyter/run_0/config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a396be5d",
   "metadata": {},
   "source": [
    "After the model has been trained, we can load it and visualize some of the planned trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4eb541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from usa.planners.clip_sdf import AStarPlanner, GradientPlanner\n",
    "\n",
    "grid_planner = AStarPlanner(\n",
    "    dataset=task._dataset(),\n",
    "    model=model.double(),\n",
    "    task=task.double(),\n",
    "    device=task._device,\n",
    "    \n",
    "    # The heuristic to use for AStar\n",
    "    heuristic=\"euclidean\",\n",
    "    # The grid resolution\n",
    "    resolution=0.1,\n",
    "    # Where to store cache artifacts\n",
    "    cache_dir=None,\n",
    ").double()\n",
    "\n",
    "# Builds the planner from the model and task.\n",
    "gradient_planner = GradientPlanner(\n",
    "    dataset=task._dataset(),\n",
    "    model=model.double(),\n",
    "    task=task.double(),\n",
    "    device=task._device,\n",
    "\n",
    "    # The learning rate for the optimizer for the waypoints\n",
    "    lr=1e-2,\n",
    "    # The weight for the total path distance loss term\n",
    "    dist_loss_weight=1.0,\n",
    "    # The weight for the inter-point distance loss term\n",
    "    spacing_loss_weight=1.0,\n",
    "    # The weight for the \"no-crashing-into-a-wall\" loss term\n",
    "    occ_loss_weight=25.0,\n",
    "    # The weight for the loss term of the final semantic location\n",
    "    sim_loss_weight=15.0,\n",
    "    # Maximum number of optimization steps\n",
    "    num_optimization_steps=1000,\n",
    "    # If points move less than this distance, stop optimizing\n",
    "    min_distance=1e-5,\n",
    "    # Where to store cache artifacts\n",
    "    # cache_dir=Path(\"cache\"),\n",
    "    cache_dir=None,\n",
    "    # Height of the floor\n",
    "    floor_height=0.1,\n",
    "    # Height of the ceiling\n",
    "    ceil_height=2.5,\n",
    ").double()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3466bd28",
   "metadata": {},
   "source": [
    "We plot the occupancy grid with the camera trajectories below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c732dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses = torch.stack([task._dataset[i].pose for i in range(len(task._dataset))])\n",
    "xs, ys = poses[..., 0, 3].numpy(), poses[..., 1, 3].numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 20))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "minx, miny = grid_planner.occ_map.origin\n",
    "(ycells, xcells), resolution = grid_planner.occ_map.grid.shape, grid_planner.occ_map.resolution\n",
    "maxx, maxy = minx + xcells * resolution, miny + ycells * resolution\n",
    "plt.imshow(grid_planner.occ_map.grid, extent=(minx, maxx, miny, maxy))\n",
    "plt.scatter(x=xs, y=ys, c='r', s=1.0)\n",
    "plt.title(\"Grid Planner\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "minx, miny = gradient_planner.occ_map.origin\n",
    "(ycells, xcells), resolution = gradient_planner.occ_map.grid.shape, gradient_planner.occ_map.resolution\n",
    "maxx, maxy = minx + xcells * resolution, miny + ycells * resolution\n",
    "plt.imshow(gradient_planner.occ_map.grid, extent=(minx, maxx, miny, maxy))\n",
    "plt.scatter(x=xs, y=ys, c='r', s=1.0)\n",
    "plt.title(\"Gradient Planner\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2928792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = gradient_planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd9d0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# waypoints = [(-5.0 + i * 0.1, 0.0) for i in range(20)]\n",
    "waypoints = planner.plan(start_xy=(-3.0, -3.0), end_xy=(3.0, 3.0))\n",
    "# waypoints = planner.plan(start_xy=(-3.0, -3.0), end_goal=\"The oven\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8be0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = zip(*waypoints)\n",
    "\n",
    "plt.figure(figsize=(10, 20))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "minx, miny = grid_planner.occ_map.origin\n",
    "(ycells, xcells), resolution = grid_planner.occ_map.grid.shape, grid_planner.occ_map.resolution\n",
    "maxx, maxy = minx + xcells * resolution, miny + ycells * resolution\n",
    "plt.imshow(grid_planner.occ_map.grid, extent=(minx, maxx, miny, maxy))\n",
    "plt.plot(xs, ys, c='r')\n",
    "plt.title(\"Grid Planner\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "minx, miny = gradient_planner.occ_map.origin\n",
    "(ycells, xcells), resolution = gradient_planner.occ_map.grid.shape, gradient_planner.occ_map.resolution\n",
    "maxx, maxy = minx + xcells * resolution, miny + ycells * resolution\n",
    "plt.imshow(gradient_planner.occ_map.grid, extent=(minx, maxx, miny, maxy))\n",
    "plt.plot(xs, ys, c='r')\n",
    "plt.title(\"Gradient Planner\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c260cdf5",
   "metadata": {},
   "source": [
    "Now that we've got our trajectory, we can run it in simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ecb81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_frames_for_waypoints(waypoints: list[tuple[float, float]], *, camera_z: float = 1.477, camera_pr: tuple[float, float, float] = (-10.0, 0.0)) -> Iterator[np.ndarray]:\n",
    "    for i in range(len(waypoints) - 1):\n",
    "        x, y = waypoints[i]\n",
    "        xn, yn = waypoints[i + 1]\n",
    "        dx, dy = xn - x, yn - y\n",
    "        # yaw = np.rad2deg(math.atan2(dy, dx))\n",
    "        yaw = np.rad2deg(math.atan2(y, x))  # Look towards origin\n",
    "        dist = math.sqrt(dx ** 2 + dy ** 2)\n",
    "        for j in np.arange(0, dist, 0.1):\n",
    "            yield capture_frame((x + dx * j, y + dy * j, camera_z), (yaw, *camera_pr))\n",
    "\n",
    "\n",
    "pkl_path = data_root / \"04_recorded_clip_waypoints.pkl\"\n",
    "write_gif(iter_frames_for_waypoints(waypoints), \"waypoints_video.gif\", pkl_path)\n",
    "Image(\"waypoints_video.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
